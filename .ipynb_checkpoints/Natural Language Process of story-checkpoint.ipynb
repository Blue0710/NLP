{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, and what is the use of a book, thought alice without pictures or conversations? so she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a white rabbit with pink eyes ran close by her. there was nothing so very remarkable in that; nor did alice think it so very much out of the way to hear the rabbit say to itself, oh dear! oh dear! i shall be late! (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge. in another moment down went alice after it, never once considering how in the world she was to get out again. the rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that alice had not a moment to think about stopping herself before she found herself falling down a very deep well. either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. first, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. she took down a jar from one of the shelves as she passed; it was labelled orange marmalade, but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, \n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fp = open(\"D:/Value of Dream/Marathon_Github/NLP/data/Alice.txt\", 'rb')\n",
    "lines = []  ## 注意  有加 s\n",
    "## strip() remove certain characters (such as whitespace) from the left and right parts of strings 抓一列  然後把最前面跟最後面的空白去掉(中間空白留著)\n",
    "## With lower() we can treat \"cat\" and \"CAT\" and \"Cat\" the same\n",
    "for line in fp:  ## 從 fp 中  逐列抓資料\n",
    "    line = line.strip().lower()  \n",
    "    line = line.decode(\"ascii\", \"ignore\")   \n",
    "    ##以 encoding 指定的編碼格式解碼字串  \"ignore\"  (just leave the character out of the Unicode result)\n",
    "    if len(line) == 0:  ## 如果抓到的資料  甚麼都沒有就繼續\n",
    "        continue\n",
    "    lines.append(line)\n",
    "fp.close()\n",
    "text = \" \".join(lines)  ##將序列中的元素以指定的字元連接生成一個新的字串  換列  應該就是用空白分隔\n",
    "print(text[800:3000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 55 \r\n",
      "字典內容：\r\n",
      " {'o', '3', 'g', 'z', '6', ']', '4', '?', ')', ';', '2', 'c', 'k', 'j', 'u', 's', 'i', 'a', 'h', '5', 'b', 'f', '0', '_', '7', 'l', '[', '(', 'y', 'x', 'r', '!', 't', '1', 'v', 'd', 'm', 'e', ',', '/', '%', '.', ' ', '#', ':', 'w', '$', '@', '*', '9', 'q', '-', 'p', 'n', '8'}\n",
      "字典索引： {'o': 0, '3': 1, 'g': 2, 'z': 3, '6': 4, ']': 5, '4': 6, '?': 7, ')': 8, ';': 9, '2': 10, 'c': 11, 'k': 12, 'j': 13, 'u': 14, 's': 15, 'i': 16, 'a': 17, 'h': 18, '5': 19, 'b': 20, 'f': 21, '0': 22, '_': 23, '7': 24, 'l': 25, '[': 26, '(': 27, 'y': 28, 'x': 29, 'r': 30, '!': 31, 't': 32, '1': 33, 'v': 34, 'd': 35, 'm': 36, 'e': 37, ',': 38, '/': 39, '%': 40, '.': 41, ' ': 42, '#': 43, ':': 44, 'w': 45, '$': 46, '@': 47, '*': 48, '9': 49, 'q': 50, '-': 51, 'p': 52, 'n': 53, '8': 54}\n",
      "index2char:{0: 'o', 1: '3', 2: 'g', 3: 'z', 4: '6', 5: ']', 6: '4', 7: '?', 8: ')', 9: ';', 10: '2', 11: 'c', 12: 'k', 13: 'j', 14: 'u', 15: 's', 16: 'i', 17: 'a', 18: 'h', 19: '5', 20: 'b', 21: 'f', 22: '0', 23: '_', 24: '7', 25: 'l', 26: '[', 27: '(', 28: 'y', 29: 'x', 30: 'r', 31: '!', 32: 't', 33: '1', 34: 'v', 35: 'd', 36: 'm', 37: 'e', 38: ',', 39: '/', 40: '%', 41: '.', 42: ' ', 43: '#', 44: ':', 45: 'w', 46: '$', 47: '@', 48: '*', 49: '9', 50: 'q', 51: '-', 52: 'p', 53: 'n', 54: '8'}\n"
     ]
    }
   ],
   "source": [
    "# 根據文本內容生成字典\n",
    "chars = set([c for c in text])  \n",
    "## 將所有字元切出來 {' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b',… 'z'}\n",
    "nb_chars = len(chars)#總共用了多少字元\n",
    "char2index = dict((c, i) for i, c in enumerate(chars))\n",
    "## enumerate() 函數用於將一個可遍歷的資料物件(如清單、元組或字串)組合為一個索引序列，同時列出資料和資料下標，一般用在 for 迴圈當中。\n",
    "## seasons = ['Spring', 'Summer', 'Fall', 'Winter']    list(enumerate(seasons, start=1))     [(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]\n",
    "## dict((c, i) for i, c in enumerate(chars))   將 c 與 i 編成字典  其中 i 是chars 裡面的字元出現順序\n",
    "\n",
    "index2char = dict((i, c) for i, c in enumerate(chars))\n",
    "print('字典大小：', nb_chars,'\\r\\n字典內容：\\r\\n' ,chars)\n",
    "print('字典索引：',char2index)\n",
    "#char2index\n",
    "print(f'index2char:{index2char}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_chars: ['project gu', 'roject gut', 'oject gute', 'ject guten', 'ect gutenb', 'ct gutenbe', 't gutenber', ' gutenberg', 'gutenbergs', 'utenbergs ', 'tenbergs a', 'enbergs al', 'nbergs ali', 'bergs alic', 'ergs alice', 'rgs alices', 'gs alices ', 's alices a', ' alices ad', 'alices adv', 'lices adve', 'ices adven', 'ces advent', 'es adventu', 's adventur', ' adventure', 'adventures', 'dventures ', 'ventures i', 'entures in', 'ntures in ', 'tures in w', 'ures in wo', 'res in won', 'es in wond', 's in wonde', ' in wonder', 'in wonderl', 'n wonderla', ' wonderlan', 'wonderland', 'onderland,', 'nderland, ', 'derland, b', 'erland, by', 'rland, by ', 'land, by l', 'and, by le', 'nd, by lew', 'd, by lewi', ', by lewis', ' by lewis ', 'by lewis c', 'y lewis ca', ' lewis car', 'lewis carr', 'ewis carro', 'wis carrol', 'is carroll', 's carroll ', ' carroll t', 'carroll th', 'arroll thi', 'rroll this', 'roll this ', 'oll this e', 'll this eb', 'l this ebo', ' this eboo', 'this ebook', 'his ebook ', 'is ebook i', 's ebook is', ' ebook is ', 'ebook is f', 'book is fo', 'ook is for', 'ok is for ', 'k is for t', ' is for th', 'is for the', 's for the ', ' for the u', 'for the us', 'or the use', 'r the use ', ' the use o', 'the use of', 'he use of ', 'e use of a', ' use of an', 'use of any', 'se of anyo', 'e of anyon', ' of anyone', 'of anyone ', 'f anyone a', ' anyone an', 'anyone any', 'nyone anyw']\n",
      "\n",
      "label_chars: ['t', 'e', 'n', 'b', 'e', 'r', 'g', 's', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a', 'n', 'd', ',', ' ', 'b', 'y', ' ', 'l', 'e', 'w', 'i', 's', ' ', 'c', 'a', 'r', 'r', 'o', 'l', 'l', ' ', 't', 'h', 'i', 's', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'i', 's', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'u', 's', 'e', ' ', 'o', 'f', ' ', 'a', 'n', 'y', 'o', 'n', 'e', ' ', 'a', 'n', 'y', 'w', 'h']\n",
      "X[0] [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "y[0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#把一篇文章變成一筆一筆資料\n",
    "SEQLEN = 10  ## 想用前10個字元去預測第 11個字元\n",
    "STEP = 1\n",
    "input_chars = []#x有10個\n",
    "label_chars = []#y有一個\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])  ## 前10個字元當作 x input\n",
    "    label_chars.append(text[i + SEQLEN])    ## 第11個字元當作 y output\n",
    "\n",
    "#input_chars\n",
    "#label_chars\n",
    "print('input_chars:', input_chars[:100])     ## show 出前 100筆資料\n",
    "print('\\r\\nlabel_chars:', label_chars[:100])\n",
    "\n",
    "\n",
    "# The next step is to vectorize these input and label texts     前面的資料是字元   現在要把xy字元依對照表  轉成數字資料\n",
    "## 先宣告整個資料的大小矩陣 X (158773(筆), 10(列), 55(行))     y (158773(筆), 55(行))  \n",
    "## 注意資料的擺放方式  每一個字元  佔一列(55行其中一個為1)  共 10 列(10個字元)  \n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)  \n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "    \n",
    "print('X[0]', X[0]*1)  ## show 出第一筆輸入變數x 資料!! (前十10個字元  每一個字元用 55個0/1 碼表示)\n",
    "print('y[0]', y[0]*1)   ## show 出第一筆輸出變數y 資料!! (第十一個字元  該字元用 55個0/1 碼表示)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 128)               23552     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 55)                7095      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 55)                0         \n",
      "=================================================================\n",
      "Total params: 30,647\n",
      "Trainable params: 30,647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 32us/step - loss: 2.3341\n",
      "Generating from seed: and then t\n",
      "and then the the said the said the said the said the said the said the said the said the said the said the sai\n",
      "\n",
      "==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 29us/step - loss: 2.0495\n",
      "Generating from seed: ly bowed, \n",
      "ly bowed, and the mast on the said the king the mast on the said the king the mast on the said the king the ma\n",
      "\n",
      "==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 29us/step - loss: 1.9478\n",
      "Generating from seed: queaking o\n",
      "queaking of the sage the was and it and and the said the dore the said the dore the said the dore the said the\n",
      "\n",
      "==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 29us/step - loss: 1.8675\n",
      "Generating from seed: you know y\n",
      "you know you the king the king the king the king the king the king the king the king the king the king the kin\n",
      "\n",
      "==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 30us/step - loss: 1.8028\n",
      "Generating from seed: ugly; and \n",
      "ugly; and the mound and the mound and the mound and the mound and the mound and the mound and the mound and th\n",
      "\n",
      "==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 30us/step - loss: 1.7489\n",
      "Generating from seed: gally requ\n",
      "gally requeen what she was she said the dormouse the dormouse the dormouse the dormouse the dormouse the dormo\n",
      "\n",
      "==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 29us/step - loss: 1.7045\n",
      "Generating from seed: ct gutenbe\n",
      "ct gutenberg-tm erout to her had to the door a mane the dormouse the dormouse the dormouse the dormouse the do\n",
      "\n",
      "==================================================\n",
      "Iteration #: 7\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 29us/step - loss: 1.6670\n",
      "Generating from seed: ps i shall\n",
      "ps i shall and had the mock turtle down and had the mock turtle down and had the mock turtle down and had the \n",
      "\n",
      "==================================================\n",
      "Iteration #: 8\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 30us/step - loss: 1.6345\n",
      "Generating from seed: en it had \n",
      "en it had the gryphon in the dormouse in the way the dormouse in the way the dormouse in the way the dormouse \n",
      "\n",
      "==================================================\n",
      "Iteration #: 9\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 29us/step - loss: 1.6064\n",
      "Generating from seed: ining behi\n",
      "ining behing and the dormouse were and the dormouse were and the dormouse were and the dormouse were and the d\n",
      "\n",
      "==================================================\n",
      "Iteration #: 10\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 34us/step - loss: 1.5814\n",
      "Generating from seed: ou, sooner\n",
      "ou, sooner and the way as it was some time the way as it was some time the way as it was some time the way as \n",
      "\n",
      "==================================================\n",
      "Iteration #: 11\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 35us/step - loss: 1.5594\n",
      "Generating from seed: t the mock\n",
      "t the mock turtle sous was the hatter would be one of the grown a look a look a look a look a look a look a lo\n",
      "\n",
      "==================================================\n",
      "Iteration #: 12\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 33us/step - loss: 1.5402\n",
      "Generating from seed: ll the unj\n",
      "ll the unjert gat so frem the mock turtle was a little stood the gryphon down and she was seres the dormouse o\n",
      "\n",
      "==================================================\n",
      "Iteration #: 13\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 30us/step - loss: 1.5223\n",
      "Generating from seed: looking ov\n",
      "looking over the dormouse some of the dormouse some of the dormouse some of the dormouse some of the dormouse \n",
      "\n",
      "==================================================\n",
      "Iteration #: 14\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 38us/step - loss: 1.5062\n",
      "Generating from seed: d the rema\n",
      "d the remark alice cat a cont on a long had not a much as the dormouse replied the dormouse replied the dormou\n",
      "\n",
      "==================================================\n",
      "Iteration #: 15\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 30us/step - loss: 1.4914\n",
      "Generating from seed:  went to w\n",
      " went to with an the door any little with any was a long the courter said the dormouse she was a long the cour\n",
      "\n",
      "==================================================\n",
      "Iteration #: 16\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 30us/step - loss: 1.4788\n",
      "Generating from seed: said the c\n",
      "said the caterpillar. alice said to herself it a little she had not a micus the dormouse a little she had not \n",
      "\n",
      "==================================================\n",
      "Iteration #: 17\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 39us/step - loss: 1.4662\n",
      "Generating from seed: t her and \n",
      "t her and she cater and she cater and she cater and she cater and she cater and she cater and she cater and sh\n",
      "\n",
      "==================================================\n",
      "Iteration #: 18\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 31us/step - loss: 1.4558\n",
      "Generating from seed:  my going \n",
      " my going to the way alice was so the way alice was so the way alice was so the way alice was so the way alice\n",
      "\n",
      "==================================================\n",
      "Iteration #: 19\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 36us/step - loss: 1.4454\n",
      "Generating from seed: ly on slat\n",
      "ly on slatenge, and she said to herself, and she said to herself, and she said to herself, and she said to her\n",
      "\n",
      "==================================================\n",
      "Iteration #: 20\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 4s 28us/step - loss: 1.4352\n",
      "Generating from seed: nner, smil\n",
      "nner, smily whin it was the same of the stod as a little shree and the rabbit had for the dormouse so the way \n",
      "\n",
      "==================================================\n",
      "Iteration #: 21\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 37us/step - loss: 1.4263\n",
      "Generating from seed: un to repe\n",
      "un to repeated the gryphon. she was the three of the three of the three of the three of the three of the three\n",
      "\n",
      "==================================================\n",
      "Iteration #: 22\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 31us/step - loss: 1.4186\n",
      "Generating from seed: g to grow \n",
      "g to grow to herself the courd to see it to see it to see it to see it to see it to see it to see it to see it\n",
      "\n",
      "==================================================\n",
      "Iteration #: 23\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 35us/step - loss: 1.4113\n",
      "Generating from seed: flamingo. \n",
      "flamingo. she took the dormouse in the dormouse in the dormouse in the dormouse in the dormouse in the dormous\n",
      "\n",
      "==================================================\n",
      "Iteration #: 24\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 30us/step - loss: 1.4033\n",
      "Generating from seed: one, or he\n",
      "one, or her hand to herself, and the three was a conce of the torm to herself, and the three was a conce of th\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build our model\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "model = Sequential()\n",
    "## input_shape=  (10(列), 55(行))\n",
    "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,input_shape=(SEQLEN, nb_chars),unroll=True))  \n",
    "model.add(Dense(nb_chars))  ## 配合輸出變數y 資料!! (55個神經元)\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "print(model.summary())\n",
    "\n",
    "##  return_sequences 意思是在每個時間點，要不要輸出output，默認的是 false。\n",
    "## 如果等於 false，就是只在最後一個時間點輸出一個值。\n",
    "##  stateful，默認的也是 false，意義是批和批之間是否有聯繫。直觀的理解就是我們在讀完二十步，\n",
    "## 第21步開始是接著前面二十步的。也就是第一個 batch中的最後一步與第二個 batch 中的第一步之間是有聯繫的。\n",
    "## unroll：布林值，默認為False，若為True，則迴圈層將被展開，否則就使用符號化的迴圈。當使用TensorFlow為後端時，迴圈網路本來就是展開的，\n",
    "## 因此該層不做任何事情。層展開會佔用更多的記憶體，但會加速RNN的運算。層展開只適用於短序列。\n",
    "\n",
    "NUM_ITERATIONS=25\n",
    "NUM_EPOCHS_PER_ITERATION=1\n",
    "                                                            \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)   ## 這樣的訓練方式跟直接epoch多一些  有何差異\n",
    "    test_idx = np.random.randint(len(input_chars))      ## len(input_chars)=158773筆資料中  產生一個筆號\n",
    "    test_chars = input_chars[test_idx]\t\t\t\t\t##  從訓練資料中  抓出該筆資料\n",
    "    print(\"Generating from seed: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):  ## 動態(添加預測資料做為 input) 往前預測NUM_PREDS_PER_EPOCH筆!!\n",
    "        Xtest = np.zeros((1, SEQLEN, nb_chars))   ## 產生一組  列=SEQLEN=10, 行= nb_chars=55   值都是 0 的矩陣資料\n",
    "        for i, ch in enumerate(test_chars):   ## list(enumerate(test_chars))  可以看!!    這一邊厲害  i ch 是跟著enumerate(test_chars)  動!!\n",
    "            Xtest[0, i, char2index[ch]] = 1   ##  把出現的字母依照其位置令其值為 1   很厲害\n",
    "        pred = model.predict(Xtest, verbose=0)[0]  ##  加了後面的[0] 可以把[1,55] 變成 [55,]   預測各行的發生機率\n",
    "\t\t## verbose 日誌顯示，0為不在標準輸出流輸出日誌信息，1為輸出進度條記錄，2為每個epoch輸出一行記錄\n",
    "        ypred = index2char[np.argmax(pred)]  ## np.argmax(pred)  找機率最大值\n",
    "        print(ypred, end=\"\")\n",
    "        # move forward with test_chars + ypred\n",
    "        test_chars = test_chars[1:] + ypred    ## 把預測值加上去  變成新的測試資料   開始往下繼續預測!!\n",
    "    print('\\r\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Importing Dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RNN\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "# ## Loading of Data\n",
    "\n",
    "text = (open(\"D:/Value of Dream/Marathon_Github/NLP/data/sonnets.txt\").read())\n",
    "text=text.lower()\n",
    "\n",
    "# ## Creating character/word mappings\n",
    "\n",
    "characters = sorted(list(set(text)))\n",
    "\n",
    "n_to_char = {n:char for n, char in enumerate(characters)}\n",
    "char_to_n = {char:n for n, char in enumerate(characters)}\n",
    "\n",
    "\n",
    "# ## Data pre-processing\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "length = len(text)\n",
    "seq_length = 100  ## 定義要用多少個前面的字元來預測\n",
    "\n",
    "## 下面開始切割資料  X 與 y\n",
    "for i in range(0, length-seq_length, 1):\n",
    "    sequence = text[i:i + seq_length]\n",
    "    label =text[i + seq_length]\n",
    "    X.append([char_to_n[char] for char in sequence])\n",
    "    Y.append(char_to_n[label])\n",
    "\n",
    "X_modified = np.reshape(X, (len(X), seq_length, 1))  ## 定義 shape for Model\n",
    "X_modified = X_modified / float(len(characters))    ## 壓縮資料在 0-1 之間\n",
    "Y_modified = np_utils.to_categorical(Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "46600/97820 [=============>................] - ETA: 19:27 - loss: 3.0076"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## A baseline model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(400, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(400))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X_modified, Y_modified, epochs=1, batch_size=100)\n",
    "\n",
    "model.save_weights('d://text_generator_400_0.2_400_0.2_baseline.h5')\n",
    "\n",
    "model.load_weights('d://text_generator_400_0.2_400_0.2_baseline.h5')\n",
    "\n",
    "\n",
    "# ## Generating Text\n",
    "\n",
    "string_mapped = X[99]\n",
    "full_string = [n_to_char[value] for value in string_mapped]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating characters\n",
    "for i in range(400):  ## 從第100 個字(X[99])開始  往下一步一步預測400 個字元\n",
    "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "    x = x / float(len(characters))\n",
    "\n",
    "    pred_index = np.argmax(model.predict(x, verbose=0)) ##預測的是索引值\n",
    "    seq = [n_to_char[value] for value in string_mapped]\n",
    "    full_string.append(n_to_char[pred_index])  ##將索引值轉成字元!! 連接起來\n",
    "\n",
    "    string_mapped.append(pred_index)\n",
    "    string_mapped = string_mapped[1:len(string_mapped)]\n",
    "\n",
    "#combining text\n",
    "txt=\"\"\n",
    "for char in full_string:\n",
    "    txt = txt+char\n",
    "txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
